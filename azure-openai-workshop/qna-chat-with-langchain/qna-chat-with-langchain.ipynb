{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Service - Chat on private data using LangChain\n",
    "\n",
    "Firstly, create a file called `.env` in this folder, and add the following content, obviously with your values:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxxxx\n",
    "OPENAI_API_BASE=https://xxxxxxx.openai.azure.com/\n",
    "```\n",
    "\n",
    "Then, let's install all dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting azure-identity==1.6.0 (from -r ../requirements.txt (line 1))\n",
      "  Using cached azure_identity-1.6.0-py2.py3-none-any.whl (108 kB)\n",
      "Requirement already satisfied: streamlit==1.18.1 in c:\\users\\mandugunduvenkateswa\\appdata\\roaming\\python\\python311\\site-packages (from -r ../requirements.txt (line 2)) (1.18.1)\n",
      "Requirement already satisfied: openai==0.27.8 in c:\\users\\mandugunduvenkateswa\\appdata\\roaming\\python\\python311\\site-packages (from -r ../requirements.txt (line 3)) (0.27.8)\n",
      "Requirement already satisfied: python-dotenv==0.21.0 in c:\\users\\mandugunduvenkateswa\\appdata\\roaming\\python\\python311\\site-packages (from -r ../requirements.txt (line 4)) (0.21.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mandugunduvenkateswa\\appdata\\roaming\\python\\python311\\site-packages (from -r ../requirements.txt (line 5)) (1.25.2)\n",
      "Requirement already satisfied: pandas in c:\\program files\\python311\\lib\\site-packages (from -r ../requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: matplotlib==3.6.3 in c:\\users\\mandugunduvenkateswa\\appdata\\roaming\\python\\python311\\site-packages (from -r ../requirements.txt (line 7)) (3.6.3)\n",
      "Requirement already satisfied: plotly==5.12.0 in c:\\users\\mandugunduvenkateswa\\appdata\\roaming\\python\\python311\\site-packages (from -r ../requirements.txt (line 8)) (5.12.0)\n",
      "Requirement already satisfied: scipy==1.10.0 in c:\\users\\mandugunduvenkateswa\\appdata\\roaming\\python\\python311\\site-packages (from -r ../requirements.txt (line 9)) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn==1.2.0 in c:\\users\\mandugunduvenkateswa\\appdata\\roaming\\python\\python311\\site-packages (from -r ../requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: tenacity in c:\\program files\\python311\\lib\\site-packages (from -r ../requirements.txt (line 11)) (8.2.2)\n",
      "Requirement already satisfied: tiktoken==0.3.0 in c:\\users\\mandugunduvenkateswa\\appdata\\roaming\\python\\python311\\site-packages (from -r ../requirements.txt (line 12)) (0.3.0)\n",
      "Requirement already satisfied: llama-index==0.4.33 in c:\\program files\\python311\\lib\\site-packages (from -r ../requirements.txt (line 13)) (0.4.33)\n",
      "Collecting langchain==0.0.129 (from -r ../requirements.txt (line 14))\n",
      "  Using cached langchain-0.0.129-py3-none-any.whl (467 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss==1.5.3 (from versions: none)\n",
      "ERROR: No matching distribution found for faiss==1.5.3\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables (set OPENAI_API_KEY and OPENAI_API_BASE in .env)\n",
    "load_dotenv()\n",
    "# Configure Azure OpenAI Service API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    "openai.api_version = \"2023-05-15\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "# Init LLM and embeddings model\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", temperature=0, openai_api_version=\"2023-05-15\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load up our documents from the `data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "loader = DirectoryLoader('../data/qna/', glob=\"*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's ingest them into FAISS so we can efficiently query our embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents=docs, embedding=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a chain that can do the whole chat on our embedding database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Adapt if needed\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                           retriever=db.as_retriever(),\n",
    "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True,\n",
    "                                           verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI is a cloud-based service by Microsoft that allows users to access OpenAI's language models, including GPT-3, Codex, and Embeddings model series. These models can be used for various natural language processing tasks such as content generation, summarization, semantic search, and natural language to code translation. The service can be accessed through REST APIs, Python SDK or the web-based interface in the Azure OpenAI Studio. The service also includes Responsible AI features, including content filtering and a use case review process to ensure the safe and ethical use of the models.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"what is azure openai service?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to easy implement chat conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is Azure OpenAI Service?\n",
      "Answer: Azure OpenAI is a service that provides access to OpenAI's advanced language models, such as GPT-3, Codex, and Embeddings, for various natural language processing tasks. The service can be used through REST APIs, Python SDK, or the Azure OpenAI Studio web-based interface. The service comes with several features, such as fine-tuning, virtual network support, responsible AI, and content filtering. To get access to the Azure OpenAI service, users must apply for access using a form available on the Azure website.\n",
      "Question: Which regions does the service support?\n",
      "Answer: The Azure OpenAI service is currently available in East US, South Central US, and West Europe.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"what is Azure OpenAI Service?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"Which regions does the service support?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-qna-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ee1bbf3137c7ea9420c4fd488a55642063e5739fe2a7286130d9ba47405b69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
