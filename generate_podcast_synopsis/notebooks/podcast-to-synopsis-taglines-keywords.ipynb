{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Podcast Synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../data/ft-interview-transcription.txt\"\n",
    "\n",
    "with open(fname,  'r', errors='replace') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# convert list to str\n",
    "content =' '.join(content) \n",
    "#print(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Set up Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = 'https://azure-openai-test21.openai.azure.com/'\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_key = 'd1869cee351446e2bc6b6ffef2207576'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a succeeded deployment of \"gpt-35-turbo\" that supports text completion with id: gpt-35-turbo.\n"
     ]
    }
   ],
   "source": [
    "# id of desired_model\n",
    "desired_model = 'gpt-35-turbo' # suitable for text generation\n",
    "desired_capability = 'completion'\n",
    "\n",
    "# list models deployed with\n",
    "deployment_id = None\n",
    "result = openai.Deployment.list()\n",
    "\n",
    "for deployment in result.data:\n",
    "    if deployment[\"status\"] != \"succeeded\":\n",
    "        continue\n",
    "    \n",
    "    model = openai.Model.retrieve(deployment[\"model\"])\n",
    "\n",
    "    # check if desired_model is deployed, and if it has 'completion' capability\n",
    "    if model[\"id\"] == desired_model and model['capabilities'][desired_capability]:\n",
    "        deployment_id = deployment[\"id\"]\n",
    "        \n",
    "# if no model deployed, deploy one\n",
    "if not deployment_id:\n",
    "    print('No deployment with status: succeeded found.')\n",
    "\n",
    "    # Deploy the model\n",
    "    print(f'Creating a new deployment with model: {desired_model}')\n",
    "    result = openai.Deployment.create(model=desired_model, scale_settings={\"scale_type\":\"standard\"})\n",
    "    deployment_id = result[\"id\"]\n",
    "    print(f'Successfully created {desired_model} that supports text {desired_capability} with id: {deployment_id}.')\n",
    "else:\n",
    "    print(f'Found a succeeded deployment of \"{desired_model}\" that supports text {desired_capability} with id: {deployment_id}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text chunks generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generator that split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def chunk_generator(text, n, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_api(document, prompt_postfix, max_tokens):\n",
    "    prompt = prompt_postfix.replace('<document>',document)\n",
    "    #print(f'>>> prompt : {prompt}')\n",
    "\n",
    "    response = openai.Completion.create(  \n",
    "    deployment_id=deployment_id, \n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=max_tokens,\n",
    "    top_p=1,\n",
    "    frequency_penalty=1,\n",
    "    presence_penalty=1,\n",
    "    stop='###')\n",
    "\n",
    "    return response['choices'][0]['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synopsis(content, prompt_postfix):\n",
    "    import tiktoken\n",
    "\n",
    "    synopsis_chunck = []\n",
    "    n = 2000 # max tokens for chuncking\n",
    "    max_tokens = 1000 # max tokens for response\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding('p50k_base')\n",
    "\n",
    "    # Generate chunkcs    \n",
    "    chunks = chunk_generator(content, n, tokenizer)\n",
    "\n",
    "    # Decode chunk of text\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "    # Request api\n",
    "    for chunk in text_chunks:\n",
    "        synopsis_chunck.append(request_api(chunk, prompt_postfix, max_tokens))\n",
    "        #print(chunk)\n",
    "        #print('>>> synopsis: \\n' + synopsis_chunck[-1])\n",
    "\n",
    "    # Synopsis\n",
    "    synopsis = ' '.join(synopsis_chunck)\n",
    "\n",
    "    return synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silicon Valley Bank (SVB) collapsed last week, the biggest bank failure since 2008. The collapse was caused by bad decisions at the bank and a rapid increase in interest rates. SVB's balance sheet had two weird features: on the deposit side they were almost entirely funded by business depositors who demand more interest when rates go up; on the lending side, all of these small companies that are Silicon Valley Bankâ€™s core customers got huge amounts of money in and deposited it at Silicon Valley Bank. So their deposits quadrupled in a couple of years, or at least tripled in a couple of years. They had so much money they couldn't even loan it out as fast as it was coming in so bought Treasury bonds which have fixed interest rates - this meant that while costs for getting money went up with rising interest rates, profits from giving out loans did not rise because those assets' prices remained fixed.\n",
      " \n",
      "The government has stepped into to ensure SVB's customers would still get their money back but there is concern about whether our banking system is secure enough. However, Robert Armstrong argues that everything should be fine so long as we don't panic because most individual banks are solvent and if you have less than $250k in your account then you're covered by US government insurance anyway.\n",
      "\n",
      "Armstrong also points out three differences between what happened now versus during 2008: first read suggests SVB looks like an outlier; there isn't necessarily a credit event per se here; big banks are really very safe this time around.\n",
      "<|im_end|> The collapse of a small US bank, First NBC Bank Holding Company, has raised concerns about the health of other banks in America. The bank's failure was caused by its rapid expansion and exposure to risky loans. However, Robert Armstrong, chief financial correspondent at the Financial Times (FT), believes that while there are risks associated with banking regulation and maturity transformation - taking short-term money in and putting long-term money out - it is unlikely that another major American bank will get into deep trouble as they have more capital than before 2008. Armstrong also suggests entrepreneurs should ask their banks whether they have plenty of capital; a variety of deposit clients; and different kinds of loans on the asset side of their balance sheet.\n",
      "  \n",
      " \n",
      "\"\"\"\n",
      "\n",
      "# Run function\n",
      "print(summarize_text(text))<|im_sep|>\n"
     ]
    }
   ],
   "source": [
    "# Prompt postfix\n",
    "prompt_postfix = \"\"\" <document>\n",
    "  \\n###\n",
    "  \\nSummarise the transcript of a podcast above into a synopsis. \n",
    "  \\nSynopsis : \n",
    "\"\"\"\n",
    "#print(prompt_postfix)\n",
    "\n",
    "synopsis = get_synopsis(content, prompt_postfix)\n",
    "\n",
    "print(synopsis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt postfix\n",
    "prompt_postfix = \"\"\" <document>\n",
    "  \\n###\n",
    "  \\nTranslate synopsis into Mandarin.  \n",
    "  \\nTranslation : \n",
    "\"\"\"\n",
    "#print(prompt_postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美国小型银行First NBC Bank Holding Company上周倒闭，这是自2008年以来最大的银行破产。该崩溃是由于该银行的错误决策和利率迅速上升造成的。SVB的资产负债表有两个奇怪的特点：在存款方面，他们几乎完全由商业存款人提供资金，当利率上涨时要求更高的利息；在放贷方面，所有这些作为硅谷银行核心客户群体中小企业得到了巨额资金，并将其存入硅谷银行。因此，在几年内他们的存款增加了四倍或至少三倍。他们有那么多钱，甚至无法像它进来一样快地发放贷款，所以购买了固定收益率国库券-这意味着尽管随着利率上涨而获取资金成本增加但给出贷款带来收益并没有增长因为那些资产价格保持不变。\n",
      "政府已经介入确保SVB客户仍然能够拿回自己的钱, 但人们对我们是否足够安全感到担忧. 然而, Robert Armstrong认为只要我们不惊慌就应该没问题, 因为大多数单个银行都是健康且如果您账户里少于$250k美元，则受到美国政府保险覆盖。\n",
      "\n",
      "Armstrong还指出现在与2008年期间发生情况之间存在三个区别:首先阅读建议SVB看起来像一个异常值; 这里并没有必然发生信用事件; 大型银行这次真正非常安全.\n",
      "\n",
      "一家小型美国 First NBC Bank Holding Company 的破产引起了关注其他美国 银 行 健 康 的 忧 虑 。 但 是 ，《 金 融 时 报》 （FT）首席财务记者罗伯特·阿姆斯特朗（Robert Armstrong）认为 ，虽然监管和成熟度转换 - 放置长期货物需要短期货物 - 存 在风险 , 但很可能不会再有另一家主要美国 银 行 沉 浸 在 深 刻 的 麻 烦 中 ，因为它们比2008年前更具备充足资本 。Armstrong 还建议企业家询问他们是否有充足的资本 ; 各种各样类型 的 存 款 客 户 ; 平衡表 上 不同 类 型 的 贷 款 .<|im_sep|>\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 1000\n",
    "translation = request_api(synopsis, prompt_postfix, max_tokens)\n",
    "print(translation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "美国金融评论家罗伯特·阿姆斯特朗（Robert Armstrong）在《金融时报》上就硅谷银行的崩溃进行了讨论，并且说明这不是2008年金融危机的复制。他解释说，导致SVB倒闭的原因有两方面：一是银行内部出现了不当决定; 二是利率急剧上升。此外，Rob 还概述了银行如何运作、SVB具体出现什么问题以及是否存在更大的体系性原因。此外，他还详细说明了为什么我们在监管方面存在双标准制度, 以及道德弗兰克法案(Dodd-Frank Act) 的退减对此情况会不会造成影响。最后,  Robert Armstrong 向听众保证, 只要人民不惊慌失措, 情况应当一切安好, 250k 美元之下的储户由美国政府承保.\n",
    "Robert Armstrong 金融专家提出了 SVB 最新崩盘及其对相关监管带来的影响。 他表明由于 2008 年危机之后所施加的法律法规使得如今的银行已然比 2008 年时更加强壮。 此外， 也告诫投者将存款存入需要留意看看能耐性情况。 最后 ， Robert Armstrong 预测随之考勒将使得 bank equity capital 更加昂���耗时曲緩效应将随之考勒使效能受到影响."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Tag Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt postfix\n",
    "prompt_postfix = \"\"\" <document>\n",
    "  \\n###\n",
    "  \\nGenerate 2 to 3 tag lines based on the podcast synopsis above.\n",
    "\"\"\"\n",
    "#print(prompt_postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "1. \"Is the US banking system secure enough? The Financial Times' Robert Armstrong weighs in.\"\n",
      "2. \"The collapse of First NBC Bank Holding Company raises concerns about other banks in America.\"\n",
      "3. \"Entrepreneurs, are you asking your bank these three questions?\"<|im_sep|>\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 500\n",
    "tag_lines = request_api(synopsis, prompt_postfix, max_tokens)\n",
    "print(tag_lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Search Engine Optimised (SEO) Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt postfix\n",
    "prompt_postfix = \"\"\" <document>\n",
    "  \\n###\n",
    "  \\nGenerate 5 search engine optimised keywords based on text above.  \n",
    "\"\"\"\n",
    "#print(prompt_postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(content, prompt_postfix):\n",
    "    import tiktoken\n",
    "\n",
    "    keywords_chunck = []\n",
    "    n = 2000 # max tokens for chuncking\n",
    "    max_tokens = 100\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding('p50k_base')\n",
    "\n",
    "    # Generate chunkcs    \n",
    "    chunks = chunk_generator(content, n, tokenizer)\n",
    "\n",
    "    # Decode chunk of text\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "    # Request api\n",
    "    for chunk in text_chunks:\n",
    "        keywords_chunck.append(request_api(chunk, prompt_postfix, max_tokens))\n",
    "\n",
    "    # Keywords\n",
    "    keywords = ' '.join(keywords_chunck)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "1. US banks: are they safe?\n",
      "2. First NBC Bank Holding Company collapse\n",
      "3. Silicon Valley Bank failure\n",
      "4. Banking regulation and maturity transformation risks \n",
      "5. Entrepreneurs' banking advice<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "keywords = get_keywords(synopsis, prompt_postfix)\n",
    "print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
